{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c408032-c93e-4137-a98e-957767d22c9d",
   "metadata": {},
   "source": [
    "# PyTorch中的Tensor基础\n",
    "\n",
    "PyTorch的tensor和Numpy的Array，Pandas的Series和Dataframe类似，是了解应用这些代码库的基础，这里首先简单快速地了解Tensor及其在PyTorch的作用。\n",
    "\n",
    "## 1 快速了解Tensor\n",
    "\n",
    "pytorch作为NumPy的替代品，可以利用GPU进行计算，是一个高灵活性、高效率的深度学习工具箱。\n",
    "\n",
    "Tensor（张量）类似于NumPy的ndarray，但还可以在GPU上使用来加速计算。因此经常能看到把numpy的数组包装为tensor再运算。tensor的操作和numpy中的数组操作类似，详见官网。下面列举一些简单例子。首先pytorch的导入是import torch，因为torch一直都是那个torch，一开始是别的语言写的，现在在python下，所以就叫pytorch。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "126a7c8f-4e05-4f70-8347-a08b2aaeefc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92c25dc-a27f-473e-98a3-52cdf01369e3",
   "metadata": {},
   "source": [
    "Tensor是pytorch的基本数据类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1abcfd79-d6ca-4589-9f6a-2e6b92bb133a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 我们经常会联合numpy使用torch，例如这里初始化0到4的张量\n",
    "x = torch.Tensor(np.arange(5))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88fa9789-b7d8-42c7-872d-7c269251b3f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.],\n",
       "        [ 3.,  4.,  5.],\n",
       "        [ 6.,  7.,  8.],\n",
       "        [ 9., 10., 11.],\n",
       "        [12., 13., 14.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建一个 5x3 的张量:\n",
    "x = torch.Tensor(np.arange(15).reshape(5, 3))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7538dfcc-1798-499f-89bb-5b845a24dbf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.],\n",
       "        [ 7.,  8.,  9.],\n",
       "        [10., 11., 12.],\n",
       "        [13., 14., 15.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 和numpy一样的广播功能\n",
    "x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2952f5-74a0-4793-8112-4a1b42d96a4b",
   "metadata": {},
   "source": [
    "PyTorch提供了Tensor，其在概念上与 numpy 数组相同，也是一个n维数组, PyTorch 也提供了很多能在这些 Tensor 上操作的函数，类似于Numpy上的函数\n",
    "\n",
    "但tensor还拥有一个numpy数组没有的功能，就是autograd自动求导，这在构建神经网络时非常重要。关于autograd的介绍可以参考Pytorch的官方文档：[A GENTLE INTRODUCTION TO TORCH.AUTOGRAD](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#:~:text=Conceptually%2C%20autograd%20keeps%20a%20record%20of%20data%20%28tensors%29,the%20input%20tensors%2C%20roots%20are%20the%20output%20tensors.)，这里我们简单谈谈其中的关键点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f228802-4f21-4c59-a4aa-fdcd33ce6e9a",
   "metadata": {},
   "source": [
    "## 2 autograd自动求导\n",
    "\n",
    "通过前面几个[B站上3Blue1Brown讲解神经网络原理的视频](https://space.bilibili.com/88461692/channel/seriesdetail?sid=1528929)，大家可以了解到神经网络的前向计算和反向传播过程，其中的梯度计算在PyTorch中就是由autograd来帮助实现的。\n",
    "\n",
    "可以说在PyTorch中，所有神经网络实现的核心就是autograd。autograd 为张量上的所有运算都提供了自动求导机制。\n",
    "\n",
    "torch.Tensor 是其核心类。如果设置它的属性 .requires_grad 为 True，那么它将会追踪对于该张量的所有运算。当完成计算后可以通过调用 .backward() 来自动计算所有梯度，并自动计入到.grad属性中\n",
    "\n",
    "还有一个类对于autograd的实现非常重要：Function。Tensor 和 Function 互相连接生成了一个有向无环图(directed acyclic graph)，就是一个树状的数据结构，它记录了完整的计算历史，这样沿着整个图，根据链式法则，就能计算梯度。Function记录在tensor中，对应其 grad_fn 属性。用户直接创建的tensor的grad_fn 是None，即新建一个tensor，它是没有grad_fn的。\n",
    "\n",
    "举个例子：构造一个tensor，设置requires_grad=True来收集梯度信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b228503-3318-4a2a-a21a-a728fca11f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b787b5-63eb-45ca-bd57-aaf48aec0d9a",
   "metadata": {},
   "source": [
    "这个tensor参与一个简单函数计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36087718-a663-4a85-8a03-5c9c6df7808f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([24., 81.], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = 3 * a**3\n",
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a78baa-9b47-4f0b-803f-e9162b082517",
   "metadata": {},
   "source": [
    "可以注意到q是一个计算结果，且它有grad_fn属性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "066e86c7-f229-45d4-8df6-84736afe26ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0a271ee-7a0f-4699-a3bb-6592dc602603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 6.0.1 (20220911.1526)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"109pt\" height=\"271pt\"\n",
       " viewBox=\"0.00 0.00 109.00 271.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 267)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-267 105,-267 105,4 -4,4\"/>\n",
       "<!-- 2150518046208 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>2150518046208</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"77.5,-31 23.5,-31 23.5,0 77.5,0 77.5,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> (2)</text>\n",
       "</g>\n",
       "<!-- 2150523744992 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>2150523744992</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"95,-86 6,-86 6,-67 95,-67 95,-86\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 2150523744992&#45;&gt;2150518046208 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2150523744992&#45;&gt;2150518046208</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-66.79C50.5,-60.07 50.5,-50.4 50.5,-41.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-41.19 50.5,-31.19 47,-41.19 54,-41.19\"/>\n",
       "</g>\n",
       "<!-- 2150523745280 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2150523745280</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"95,-141 6,-141 6,-122 95,-122 95,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">PowBackward0</text>\n",
       "</g>\n",
       "<!-- 2150523745280&#45;&gt;2150523744992 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>2150523745280&#45;&gt;2150523744992</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-121.75C50.5,-114.8 50.5,-104.85 50.5,-96.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-96.09 50.5,-86.09 47,-96.09 54,-96.09\"/>\n",
       "</g>\n",
       "<!-- 2150523745136 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2150523745136</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-196 0,-196 0,-177 101,-177 101,-196\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 2150523745136&#45;&gt;2150523745280 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>2150523745136&#45;&gt;2150523745280</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-176.75C50.5,-169.8 50.5,-159.85 50.5,-151.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-151.09 50.5,-141.09 47,-151.09 54,-151.09\"/>\n",
       "</g>\n",
       "<!-- 2150518046048 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>2150518046048</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"77.5,-263 23.5,-263 23.5,-232 77.5,-232 77.5,-263\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\"> (2)</text>\n",
       "</g>\n",
       "<!-- 2150518046048&#45;&gt;2150523745136 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2150518046048&#45;&gt;2150523745136</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-231.92C50.5,-224.22 50.5,-214.69 50.5,-206.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-206.25 50.5,-196.25 47,-206.25 54,-206.25\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1f4b5343310>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6679efb-e25f-478b-aa22-db100df8d42b",
   "metadata": {},
   "source": [
    "蓝色叶节点就是输入张量，绿色根节点是输出张量，灰色中间计算结果张量每一步都会被记录下来，并且有对应grad_fn\n",
    "\n",
    "现在我们简单看下反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c3887c5-e23d-4366-8d1a-a73cfe412007",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "q.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03ba009b-8953-4ab4-94e4-88446d392f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9*a**2 == a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d936d1-e3f9-4539-8333-3bf136c63c80",
   "metadata": {},
   "source": [
    "可以看到，q关于a的梯度是正确的。\n",
    "\n",
    "此外，为了阻止跟踪历史，可以使用 `with torch.no_grad():` 包裹代码块，例如评价模型时，我们不要再训练模型，这时候就很有用，后面我们有一个完整实例。\n",
    "\n",
    "综上：\n",
    "\n",
    "- 在PyTorch中，torch.Tensor是存储和变换数据的主要工具。\n",
    "- Tensor与Numpy的多维数组非常相似。\n",
    "- Tensor还提供了GPU计算和自动求梯度等更多功能，这些使Tensor更适合深度学习。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tutorial)",
   "language": "python",
   "name": "tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
